{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f253d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.figure_factory as ff\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b761f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08055a10",
   "metadata": {},
   "source": [
    "# Ссылки:\n",
    "- https://habr.com/ru/company/stepic/blog/250527/ - статья на хабре"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c032dc82",
   "metadata": {},
   "source": [
    "# Основная идея - вероятностный подход\n",
    "\n",
    "**В случае с вероятностным подходом ученый, получивший в своих наблюдениях численные данные, задается вопросом о вероятности получить точно такие же данные в случае, когда результат на самом деле случаен (то есть искомой закономерности в данных нет).** \n",
    "\n",
    "Достоверными в области биологии, психологии и ряда иных дисциплин считаются результаты, характеризуемые вероятностью p < 0,05: эта запись означает, что **вероятность случайного результата составляет не более пяти процентов.**\n",
    "\n",
    "**Статистическая гипотеза** - это предположение о каких-либо характеристиках случайной величины\n",
    "\n",
    "**Значение p-уровня значимости (p-value)** — это вероятность получить в своей выборке данные, которые говорят о какой-либо закономерности при условии, что в генеральной совокупности никакой закономернотси на самом деле нет. **вероятность получить обозреваемые данные при верной нулевой гипотезе**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a038dc",
   "metadata": {},
   "source": [
    "# Умозрительный пример 1\n",
    "\n",
    "X = [1,2,3,4] и Y = [2,4,6,8]\n",
    "\n",
    "В общем случае гипотеза H0 гласит, что две переменные(X и Y) независимы, т. е. Изменение значений в X не повлияет на значения в Y.\n",
    "\n",
    "Если вы вычислите \"p-value\", используя любой метод для этого случая, то оно должно оказаться очень малым значением, подразумевая, что существует очень низкая вероятность того, что этот случай будет следовать гипотезе H0, т. е. очень низкая вероятность того, что X и Y независимы друг от друга.\n",
    "\n",
    "Это означает, что он никогда не будет следовать гипотезе H0 здесь, и эти две переменные зависят друг от друга в форме Y = 2X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bad6d2",
   "metadata": {},
   "source": [
    "# Умозрительный пример 2\n",
    "\n",
    "Предположим, мы решили выяснить, существует ли взаимосвязь между пристрастием к кофе и агрессивностью у школьников. Для этого были случайным образом сформированы две группы школьников по 100 человек в каждой (1 группа — дети выпивающие 0.3 литра кофе, вторая группа — совсем ну употребляющие кофе). В качестве показателя агрессивности выступает, например, число драк со сверстниками. И допустим, **нам видно**, что кофеманы дерутся чаще. Как проверить что обозреваеммый результат статистически значим?\n",
    "\n",
    "В нашем случае...\n",
    "\n",
    "H0: Между выборками пьющих и не пьющих кофе школьников нет статистически значимых различий.\n",
    "\n",
    "H1: Между выборками есть статистически значимые различия.\n",
    "\n",
    "**p-value** - это вероятность того, что гипотеза h0 подтверждается на имеющихся данных. Вероятность наблюдения рассматриваемых данных при верной гипотезе H0.\n",
    "\n",
    "\n",
    "Мы сравнили две группы школьников между собой по уровню агрессивности при помощи стандартного t-теста (или непараметрического критерия Хи — квадрат более уместного в данной ситуации) и получили, что заветный p-уровень значимости меньше 0.05 (например 0.04). \n",
    "\n",
    "**p-value = 0.04 < alpha = 0.05 => отвергаем H0**\n",
    "\n",
    "Но о чем в действительности говорит нам полученное значение p-уровня значимости?\n",
    "\n",
    "1. **Кофеин — причина агрессивного поведения с вероятностью 96%.**\n",
    "\n",
    "Это пример ошибки корреляции: факт значимой взаимосвязи двух переменных ничего не говорит нам о причинах и следствиях. Может быть, это более агрессивные дети имеют склонность к кофеину, а вовсе не кофеин делает детей агресивными.\n",
    "\n",
    "\n",
    "2. **Вероятность того, что агрессивность и употребелние кофе не связаны, равна 0.04.**\n",
    "\n",
    "Тоже нет. Все дело в том, что мы изначально принимаем за данное, что никаких различий на самом деле нет. И, держа это в уме как факт, рассчитываем значение p-value. Поэтому правильная интерпретация: **«Если предположить, что агрессивность и употребление кофе никак не связаны, то вероятность получить такие или еще более выраженные различия составила 0.04».**\n",
    "\n",
    "\n",
    "3. **Если бы мы получили p-уровень значимости больше, чем 0.05, это означало бы, что агрессивность и употребление кофе никак не связаны между собой.**\n",
    "\n",
    "Нет, это означает лишь то, что различия, может быть, и есть, но наши результаты не позволили их обнаружить.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Итак еще раз - вывод: **Если предположить, что агрессивность и употребление кофе никак не связаны, то вероятность получить такие или еще более выраженные различия составила 0.04. Значит, кофе и уровень агрессии связаны между собой.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b9337",
   "metadata": {},
   "source": [
    "# Критерий Хи-квадрат\n",
    "\n",
    "http://www.machinelearning.ru/wiki/index.php?title=%D0%9A%D1%80%D0%B8%D1%82%D0%B5%D1%80%D0%B8%D0%B9_%D1%85%D0%B8-%D0%BA%D0%B2%D0%B0%D0%B4%D1%80%D0%B0%D1%82\n",
    "\n",
    "## Этого критерия есть два возможных применения:\n",
    "- **Определение распределения случайной величины.** (Проверка гипотезы, что рассматриваемая выборка извлечена из генеральной совокупности заданого закона распределения)\n",
    "- **Определение значимой корреляции между двумя категориальными переменными.** Проверка гипотезы об отсутствии связи между двумя переменными.\n",
    "\n",
    "\n",
    "\n",
    "### Чем отличаются критерии Хи-квадрат и Хи-квадрат-Пирсона? Ничем"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598b302",
   "metadata": {},
   "source": [
    "# Пример с кубиком"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ad223b",
   "metadata": {},
   "source": [
    "### Мы провели 4 серии экспериментов с подбрасыванием кубика и получили следующие результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd477cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = pd.DataFrame({\n",
    "    '': ['one', 'two', 'three', 'four', 'five', 'six'],\n",
    "    'a': [6, 8, 5, 4, 5, 7],\n",
    "    'b': [4, 5, 4, 11, 8, 3],\n",
    "    'c': [5, 3, 8, 7, 7, 5],\n",
    "    'd': [10, 3, 4, 13, 6, 9]\n",
    "})\n",
    "\n",
    "dice = dice.set_index('')\n",
    "dice.loc[\"total_rolls\"] = dice.sum() # добавляем суммирующую строку\n",
    "dice['total_dist'] = dice.sum(axis=1) # добавляем суммирующую колонку\n",
    "\n",
    "dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c00651",
   "metadata": {},
   "source": [
    "### Как видно 4 выпадала гораздо чаще остальных, а 2 и 3 наоборот реже. \n",
    "### Свзано ли это с тем, что кубик неправильный? Или же наша выборка недостаточно большая и такие результаты случайны?\n",
    "### Используем критерий Хи-квадрат"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f9e30c",
   "metadata": {},
   "source": [
    "#### Вначале нужно перевести результаты в формат np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ceda8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['one', 'two', 'three', 'four', 'five', 'six']\n",
    "columns = ['a', 'b', 'c', 'd']\n",
    "\n",
    "dice = np.array(dice.loc[index, columns])\n",
    "dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadaf93e",
   "metadata": {},
   "source": [
    "#### Или же просто"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cf96b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = [6, 4, 5, 10]\n",
    "a2 = [8, 5, 3, 3]\n",
    "a3 = [5, 4, 8, 4]\n",
    "a4 = [4, 11, 7, 13]\n",
    "a5 = [5, 8, 7, 6]\n",
    "a6 = [7, 3, 5, 9]\n",
    "\n",
    "dice = np.array([a1, a2, a3, a4, a5, a6])\n",
    "dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3ed117",
   "metadata": {},
   "source": [
    "### В случае, когда у нас нет предположений о распределении случайной величины, можно использовать метод stat.chi2_contigency. Он вернет частотную таблицу, которая поможет составить некоторое представление о распределении выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_square_statistic, p_value, degrees_of_freedom, expected_frequencies = stats.chi2_contingency(dice)\n",
    "\n",
    "print(\n",
    "f'''\n",
    "\n",
    "chi_square_statistic: {chi_square_statistic}\n",
    "\n",
    "p_value: {p_value}\n",
    "\n",
    "degrees_of_freedom: {degrees_of_freedom}\n",
    "\n",
    "\n",
    " === Contingency Table - expected_frequencies ===\n",
    "\n",
    "{expected_frequencies}\n",
    "\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dd18f5",
   "metadata": {},
   "source": [
    "### Вероятность получить такие результаты при верной нулевой гипотезе - 0.35, что больше чем пороги ошибки 0.05 и 0.01. Следовательно, мы подверждаем нуливую гипотезу. Кубик нормальный. По крайней мере, такой вывод можно исходит из наших данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0096a67",
   "metadata": {},
   "source": [
    "### Теперь сделаем тоже самое, но с большей выборкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ac639",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = np.random.randint(1,7,1000)\n",
    "r2 = np.random.randint(1,7,1000)\n",
    "r3 = np.random.randint(1,7,1000)\n",
    "r4 = np.random.randint(1,7,1000)\n",
    "r5 = np.random.randint(1,7,1000)\n",
    "\n",
    "unique, counts1 = np.unique(r1, return_counts=True)\n",
    "unique, counts2 = np.unique(r2, return_counts=True)\n",
    "unique, counts3 = np.unique(r3, return_counts=True)\n",
    "unique, counts4 = np.unique(r4, return_counts=True)\n",
    "unique, counts5 = np.unique(r5, return_counts=True)\n",
    "\n",
    "dice = np.array([counts1, counts2, counts3, counts4, counts5])\n",
    "\n",
    "chi_square_statistic, p_value, degrees_of_freedom, expected_frequencies = stats.chi2_contingency(dice)\n",
    "\n",
    "print(\n",
    "f'''\n",
    "\n",
    "chi_square_statistic: {chi_square_statistic}\n",
    "\n",
    "p_value: {p_value}\n",
    "\n",
    "degrees_of_freedom: {degrees_of_freedom}\n",
    "\n",
    "\n",
    " === Contingency Table - expected_frequencies ===\n",
    "\n",
    "{expected_frequencies}\n",
    "\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f232a3",
   "metadata": {},
   "source": [
    "### Чем больше наша выборка тем больше частоты в таблице соответствуют равномерному распределению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd498911",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = np.random.randint(1,7, 10000)\n",
    "r2 = np.random.randint(1,7, 10000)\n",
    "r3 = np.random.randint(1,7, 10000)\n",
    "r4 = np.random.randint(1,7, 10000)\n",
    "r5 = np.random.randint(1,7, 10000)\n",
    "\n",
    "unique, counts1 = np.unique(r1, return_counts=True)\n",
    "unique, counts2 = np.unique(r2, return_counts=True)\n",
    "unique, counts3 = np.unique(r3, return_counts=True)\n",
    "unique, counts4 = np.unique(r4, return_counts=True)\n",
    "unique, counts5 = np.unique(r5, return_counts=True)\n",
    "\n",
    "dice = np.array([counts1, counts2, counts3, counts4, counts5])\n",
    "\n",
    "chi_square_statistic, p_value, degrees_of_freedom, expected_frequencies = stats.chi2_contingency(dice)\n",
    "\n",
    "print(\n",
    "f'''\n",
    "\n",
    "chi_square_statistic: {chi_square_statistic}\n",
    "\n",
    "p_value: {p_value}\n",
    "\n",
    "degrees_of_freedom: {degrees_of_freedom}\n",
    "\n",
    "\n",
    " === Contingency Table - expected_frequencies ===\n",
    "\n",
    "{expected_frequencies}\n",
    "\n",
    "1/6 * 10 000 = {1/6*10000}\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d8e05",
   "metadata": {},
   "source": [
    "### Теперь допустим, что у нас есть предположение о распределении случайной величины. Мы рассматриваем кубик - следовательно результаты должны быть распределены равномерно. Посчитав характеристики нашей выборки можно сгенерить теоретическое распределение. После чего сравнить эмпирическую и теоретическую выборки\n",
    "### H0: результаты распределены равномерно - кубик нормальный\n",
    "### H1: результаты распределены неравномерно - кубик бракованный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41aa7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([59, 63, 37, 38, 32, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f74b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([59, 63, 37, 38, 32, 50])/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46846a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rolls_expected = [46.5, 46.5, 46.5, 46.5, 46.5, 46.5] # теоретическое ожидаемое распределение\n",
    "my_rolls_actual =  [46.5, 46.5, 46.5, 46.5, 46.5, 46.5] # эмпирическое наблюдаемое распределение\n",
    "\n",
    "stats.chisquare(my_rolls_actual, my_rolls_expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9823be",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rolls_expected = [46.5, 46.5, 46.5, 46.5, 46.5, 46.5] # теоретическое ожидаемое распределение\n",
    "my_rolls_actual =  [59, 63, 37, 38, 32, 50] # эмпирическое наблюдаемое распределение\n",
    "\n",
    "stats.chisquare(my_rolls_actual, my_rolls_expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rolls_expected_normalized = [1/6]*6 # теоретическое ожидаемое распределение\n",
    "my_rolls_actual_normalized =  [val/sum(my_rolls_actual) for val in my_rolls_actual] # эмпирическое наблюдаемое распределение\n",
    "\n",
    "stats.chisquare(my_rolls_expected_normalized, my_rolls_actual_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd17fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "?stats.chisquare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1432bfba",
   "metadata": {},
   "source": [
    "### p-value = 0.003. Вероятность получения эмпирической выборки меньше порога ошибки, равного 0.01. Мы отвергаем нулевую гипотезу"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3886a6c7",
   "metadata": {},
   "source": [
    "# Игрушечный пример 1\n",
    "## - определить распределение случайной величины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['CDU', 0.415, 57], \n",
    "        ['SPD', 0.257, 26], \n",
    "        ['Others', 0.328, 40]]\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['Varname', 'prob_dist', 'observed_freq']) \n",
    "df['expected_freq'] = df['observed_freq'].sum() * df['prob_dist']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab00e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Calcualtion of Chisquare test statistics\n",
    "chi_square = 0\n",
    "for i in range(len(df)):\n",
    "    O = df.loc[i, 'observed_freq']\n",
    "    E = df.loc[i, 'expected_freq']\n",
    "    chi_square += (O-E)**2/E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b9266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The p-value approach\n",
    "print(\"Approach 1: The p-value approach to hypothesis testing in the decision rule\")\n",
    "p_value = 1 - stats.norm.cdf(chi_square, df['Varname'].nunique() - 1)\n",
    "conclusion = \"Failed to reject the null hypothesis.\"\n",
    "if p_value <= alpha:\n",
    "    conclusion = \"Null Hypothesis is rejected.\"\n",
    "        \n",
    "print(\"chisquare-score is:\", chi_square, \" and p value is:\", p_value)\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d20bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The critical value approach\n",
    "\n",
    "print(\"Approach 2: The critical value approach to hypothesis testing in the decision rule\")\n",
    "critical_value = stats.chi2.ppf(1-alpha, df['Varname'].nunique() - 1)\n",
    "conclusion = \"Failed to reject the null hypothesis.\"\n",
    "if chi_square > critical_value:\n",
    "    conclusion = \"Null Hypothesis is rejected.\"\n",
    "        \n",
    "print(\"chisquare-score is:\", chi_square, \" and p value is:\", critical_value)\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06378147",
   "metadata": {},
   "source": [
    "##### Здесь я пока не понимаю"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7859c96d",
   "metadata": {},
   "source": [
    "# Игрушечный пример 2\n",
    "## - проверить наличие связи между двумя категориальными переменными"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f0da8",
   "metadata": {},
   "source": [
    "### Допустим мы хотим проследить связь пола человека с его пристрастием к курению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3145dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Gender' : ['M', 'M', 'M', 'F', 'F'] * 10,\n",
    "                   'isSmoker' : ['Smoker', 'Smoker', 'Non-Smpoker', 'Non-Smpoker', 'Smoker'] * 10\n",
    "                  })\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e8536",
   "metadata": {},
   "source": [
    "### Для проведения теста Хи-квадрат, нужно создать частотную перекресную таблицу "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "contigency= pd.crosstab(df['Gender'], df['isSmoker']) \n",
    "contigency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b6d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "contigency_pct = pd.crosstab(df['Gender'], df['isSmoker'], normalize='index') # normalize='column', normalize='all' \n",
    "contigency_pct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cc1c8b",
   "metadata": {},
   "source": [
    "### Для удобства можно построить heatmap. Пользуясь функционалом styler или seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82aea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "contigency.style.background_gradient(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c638095",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8)) \n",
    "sns.heatmap(contigency, annot=True, cmap=\"YlGnBu\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba71b5e",
   "metadata": {},
   "source": [
    "### Применяем критерий Хи-квадрат\n",
    "\n",
    "- с - статистика теста\n",
    "- p - p-value\n",
    "- dof -степень свободы\n",
    "- expected - ожидаемые частоты, основанные на предельных суммах таблицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a53b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "c, p, dof, expected = stats.chi2_contingency(contigency)\n",
    "\n",
    "print(f'c: {c}\\n \\np: {p} \\n \\ndof: {dof} \\n \\nexpected: {expected}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f96ea2",
   "metadata": {},
   "source": [
    "### p-value = 37.6%, это значит, что мы принимаем нулевую гипотезу о не связанности пола с пристрастием к курению с 95% уровнем уверенности\n",
    "\n",
    "#### Можно ли в данном случае применить z-test?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f27de",
   "metadata": {},
   "source": [
    "# Визуализация распределений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e74a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "reactions = [328,454,312,625,609,546,502,736,485,766,429,313,328,344,360,\n",
    "             453,563,343,375, 28,312,361,297,437,328,328,328,297,359,328,\n",
    "             361,703,500,344,329,312,328,547,314,328,439,359,126,408,360,\n",
    "             346,328,392,453,359]\n",
    "\n",
    "samples=np.array(reactions)\n",
    "mean=np.mean(reactions)\n",
    "var=np.var(reactions)\n",
    "std=np.sqrt(var) # квадратный корень из дисперсии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37c24ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(min(samples), max(samples), 12)\n",
    "print(\"Excess kurtosis of normal distribution ( should be 0): {}\".format(stats.kurtosis(x)))\n",
    "print(\"Skewness of normal distribution ( should be 0): {}\".format(stats.skew(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dae0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pdf = stats.norm.pdf(x, mean, std)\n",
    "y_skew_pdf = stats.skewnorm.pdf(x, *stats.skewnorm.fit(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c2cd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, = plt.plot(x, y_pdf, label='PDF')\n",
    "l2, = plt.plot(x, y_skew_pdf, label='SKEW PDF')\n",
    "\n",
    "# Compute histogram of Samples\n",
    "n, bins, patches =plt.hist(samples,\n",
    "                           bins = 12,\n",
    "                           density=True,\n",
    "                           \n",
    "                           facecolor='g',\n",
    "                           edgecolor='red', \n",
    "                           alpha=0.75)\n",
    "\n",
    "plt.axvline(label='Mean=404.2 ms',x=404.2,linestyle='dashed')\n",
    "plt.axvline(label='Mean-2sigma=158.06 ms',x=158.06,linestyle='dashed')\n",
    "plt.axvline(label='Mean+2sigma=650.34 ms',x=650.34, linestyle='dashed')\n",
    "\n",
    "plt.xlabel('My Reaction Time (ms)')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Histogram of reaction to visual stimuli')\n",
    "\n",
    "# The first plt.text arguments are coordinates x,y of the plot\n",
    "plt.text(400, 0.015,r'mean=404.2,sigma=123.07')\n",
    "plt.legend((l1,l2),(l1.get_label(), l2.get_label()), loc='upper right')\n",
    "plt.axis([126, 766, 0, 0.01])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2815b43c",
   "metadata": {},
   "source": [
    "### Как сделать тоже самое с plotly?\n",
    "https://plotly.com/python/distplot/ - ссылка на документацию по построению гистограмм в plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f6a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "x = np.random.normal(loc=2.5, scale=0.85, size=300) \n",
    "group_labels = 'My sample'\n",
    "\n",
    "# Create distplot with custom bin_size, and without rug plot\n",
    "fig = ff.create_distplot([x], [group_labels], bin_size=.2, show_rug=False)\n",
    "fig.update_layout(width=600, \n",
    "                  height=400,\n",
    "                  bargap=0.01)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbbf575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = px.data.tips()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66588472",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x=\"total_bill\", y=\"tip\", color=\"sex\", marginal=\"rug\",\n",
    "                   hover_data=df.columns)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'2012': np.random.randn(200),\n",
    "                   '2013': np.random.randn(200)+1})\n",
    "fig = ff.create_distplot([df[c] for c in df.columns], df.columns, bin_size=.25)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce47e10",
   "metadata": {},
   "source": [
    "# Теперь посмотрим на реальные данные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c78945",
   "metadata": {},
   "source": [
    "## Зачем нам знать распределение случайной величины?\n",
    "\n",
    "#### Пример с койкой.\n",
    "Если мы знаем, что рост людей подчиняется нормальному распределению, мы можем, например оценить возможную величину спроса на кравати определенной длинны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('data/input/titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b611110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = titanic.fillna(0)\n",
    "titanic = titanic.rename(columns={'2urvived': 'Survived'})\n",
    "titanic = titanic.drop([col for col in titanic.columns if 'zero' in col], axis=1)\n",
    "\n",
    "print(f'Shape: {titanic.shape[0]}')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fba283",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = titanic.describe()\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92e9409",
   "metadata": {},
   "source": [
    "## Проверка распределения случайной величины \"Survived\" (выжил или нет)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93340b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['Survived'].plot.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d429dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "report['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b1e179",
   "metadata": {},
   "source": [
    "### Распределение Бернулли"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be52897c",
   "metadata": {},
   "source": [
    "#### Сгенерим теоретическую выборку с распределением Бернулли"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bernoulli\n",
    "\n",
    "p = titanic[titanic['Survived'] == 1].shape[0]/titanic['Survived'].count()\n",
    "print(f'p: {p}\\n')\n",
    "survived = bernoulli(p)\n",
    "\n",
    "print(f'Вероятность значения 1: {np.round(survived.pmf(1),2)}')\n",
    "print(f'Вероятность значения 0: {np.round(survived.pmf(0), 2)}', end='\\n \\n')\n",
    "\n",
    "survived_samples = survived.rvs(titanic.shape[0])\n",
    "survived_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f882d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_theoretical = pd.Series(survived_samples)\n",
    "vc_theoretical = pd.concat([\n",
    "    x_theoretical.value_counts(), \n",
    "    x_theoretical.value_counts(normalize=True)\n",
    "], axis=1).rename(columns={0: 'counts', 1: 'normalized'})\n",
    "\n",
    "y_theiretical = x_theoretical.apply(lambda x: vc_theoretical.loc[x])\n",
    "print('theoretical dist:')\n",
    "vc_theoretical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d437d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_empirical = titanic['Survived']\n",
    "vc_empirical = pd.concat([\n",
    "    x_empirical.value_counts().rename('counts'), \n",
    "    x_empirical.value_counts(normalize=True).rename('normalized')\n",
    "], axis=1)\n",
    "\n",
    "y_empirical = x_empirical.apply(lambda x: vc_empirical.loc[x])\n",
    "print('empirical dist:')\n",
    "vc_empirical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2102ec35",
   "metadata": {},
   "source": [
    "#### Как мы видим, теоретическое и эмпирическое распределение отличаются. Но, достаточны ли эти различия, чтобы считать, что эмпирическая выборка имеет другое распределение?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4600ae",
   "metadata": {},
   "source": [
    "#### H0: Выборки распределены одинаковым образом. Эмпирическая выборка имеет распределение Бернули\n",
    "#### H1: Распределения выборок не совпадают. Эмпирическая выборка имеет распределение отличное от Бернули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0848d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.chisquare(vc_empirical.values[0], vc_theoretical.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbade7d",
   "metadata": {},
   "source": [
    "#### p-value = 1 > 0.05. Вероятность получить эмпирические данные при верной нулевой гипотезе больше чем порог ошибки. Мы подтверждаем нулевую гипотезу. На основе имеющихся данных можно сделать вывод, что значения колонки survived распределены по Бернули. \n",
    "\n",
    "#### 100%-ое распределение Бернулли, как и ожидалось"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6fbc07",
   "metadata": {},
   "source": [
    "## Проверка распределения случайной величины \"Age\" (возраст)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "report['Age']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f65d8af",
   "metadata": {},
   "source": [
    "#### Итак, у нас на руках непрерывная величина. А Хи-квадрат работает только с дискретными. И при том каждый из исходов должен насчитывать пять или больше вхождений. Поэтому имеет смысл от конкретных возрастов перейти к возрастным диапазонам. И в дальнейшем смореть распределение уже по этим диапазонам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b4eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = titanic['Age']\n",
    "mean = report['Age']['mean']\n",
    "std = report['Age']['std']\n",
    "bins_num = 6 # выбираем количество диапазонов равное 6, чтобы каждый насчитывал хотя бы 5 случаев\n",
    "\n",
    "obs_ns, obs_bins, obs_patches = plt.hist(obs, \n",
    "                                         bins = bins_num, \n",
    "                                         density=False, \n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c111e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n*np.diff(bins) # Быстрый способ посчитать площадь под бинами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1b5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(obs_ns))\n",
    "obs_ns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35907f57",
   "metadata": {},
   "source": [
    "### 1) Нормальное распределение \n",
    "- Среднее значение = 29.5\n",
    "- Стандартное отклонение = 12.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41bcc8",
   "metadata": {},
   "source": [
    "### Использование функции теоретической вероятности\n",
    "\n",
    "F_Xi(x) = P(Xi<x)\n",
    "\n",
    "Интеграл _ -Inf .. x p_Xi(x) dx\n",
    "\n",
    "P(a<x<b) = F_Xi(b) - F_Xi(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9254ffff",
   "metadata": {},
   "source": [
    "#### Теперь нужно посчитать распределение частот по нашим диапазонам для нормально распределенной случайной величины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8614ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies(obs_bins, obs_size, theoretical):\n",
    "\n",
    "    values = []\n",
    "    probabilities = []\n",
    "\n",
    "    for start, end in zip(obs_bins[:-1], obs_bins[1:]):\n",
    "        # разность значений кумулятивной функции плотности от начала и конца диапазона\n",
    "        # - это вероятность попадания с.в. в этот диапазон\n",
    "        probabilities.append(theoretical.cdf(end) - theoretical.cdf(start)) \n",
    "\n",
    "        # считаем среднее значение меду началами и концами диапазонов, чтобы потом построить график\n",
    "        values.append((end+start)/2)\n",
    "        \n",
    "    # Умножаем полученные вероятности попадания на размер нашей обозреваемой выборки, чтобы получить частоты\n",
    "    frequencies = np.array(probabilities)*obs_size\n",
    "    return frequencies, values\n",
    "    \n",
    "norm = stats.norm(mean, std)\n",
    "exp_ns, values = get_frequencies(obs_bins, len(obs), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d60fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_line, = plt.plot(values, obs_ns, label='observed', c = 'blue')\n",
    "exp_line, = plt.plot(values, exp_ns, label='norm', c = 'red')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149147c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'observed sum: {sum(obs_ns)}')\n",
    "print(f'expected sum: {sum(exp_ns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b345a6",
   "metadata": {},
   "source": [
    "#### По непонятным причинам, в отличае от обозреваемых частот, сумма теоретических не равна размеру выборки. Критерий требует, чтобы суммы обозреваемых и теоретических частот практически не отличались. Поэтому я решил нормализовать частоты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(array, val):\n",
    "    # высчитываю недостающее количество\n",
    "    # и расделяю его равномерно между всеми элементами масива\n",
    "    # на всякий случай округляю все значения\n",
    "    array = np.round(array + ((val-sum(array))/len(array)))\n",
    "    return array\n",
    "\n",
    "obs_ns_normalized = normalize(obs_ns, val=len(obs)) # для сохранения однородности\n",
    "exp_ns_normalized = normalize(exp_ns, val=len(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(obs_ns_normalized))\n",
    "print(sum(exp_ns_normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46770dcb",
   "metadata": {},
   "source": [
    "#### Чаcтоты совпадают и их можно тестировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f5f4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_ns_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_ns_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2532d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.chisquare(obs_ns_normalized, exp_ns_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afeccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(stats.chi2.ppf(0.95, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb87d9",
   "metadata": {},
   "source": [
    "#### И значение p-value приближени к нулю. Статистика намного выше значения из таблицы при уровне значимости 0.05 и степени свободы 5, что также позволяет нам уверенно отвергнуть нулевую гипотезу. Слишком уверено "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ed6e3",
   "metadata": {},
   "source": [
    "### 2) Логиситическое распределение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lognorm = stats.lognorm(mean, std)\n",
    "exp_ns, values = get_frequencies(obs_bins, len(obs), lognorm)\n",
    "\n",
    "obs_line, = plt.plot(values, obs_ns, label='observed', c = 'blue')\n",
    "exp_line, = plt.plot(values, exp_ns, label='lognorm', c = 'red')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4071043",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_ns_normalized = normalize(obs_ns, val=len(obs)) # для сохранения однородности\n",
    "exp_ns_normalized = normalize(exp_ns, val=len(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f337671",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b945b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(obs_ns_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5936746",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(exp_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44259d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(exp_ns_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7066fe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.chisquare(obs_ns_normalized, exp_ns_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4131a32",
   "metadata": {},
   "source": [
    "### 3) Распределение Пуасона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8959e6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
